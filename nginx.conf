# ============================================
# Global Settings
# ============================================
user nginx;
worker_processes auto; # Auto-detect number of CPU cores
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

# ============================================
# Events Block
# Configures connection processing
# ============================================
events {
    worker_connections 1024; # Max simultaneous connections per worker
    # Total max connections = worker_processes * worker_connections
}

# ============================================
# HTTP Block
# ============================================
http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    # --------------------------------------------
    # Logging Format
    # --------------------------------------------
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'upstream=$upstream_addr upstream_time=$upstream_response_time';
    # $remote_addr: Client IP
    # $status: HTTP status code (200, 404, 500, etc.)
    # $upstream_addr: Which backend server handled the request
    # $upstream_response_time: How long the backend took
    access_log /var/log/nginx/access.log main;
    # --------------------------------------------
    # Performance Settings
    # --------------------------------------------
    sendfile on; # Efficient file serving
    tcp_nopush on; # Send headers in one packet
    tcp_nodelay on; # Don't buffer small packets
    keepalive_timeout 65; # Time to keep connections alive
    types_hash_max_size 2048; # Max size of MIME types hash
    # --------------------------------------------
    # Upstream Block
    # Defines the pool of backend servers 
    # --------------------------------------------
    upstream ml_workers {
        # Load balancing method (optional, default is round-robin)
        # least_conn; #Use this to send requests to servers with the least active connections

        # List of backend servers
        # Format: server <IP_ADDRESS>:<PORT> [parameters];
        server worker:8000 max_fails=3 fail_timeout=30s;
        # max_fails: Mark server as down after 3 failed health checks
        # fail_timeout: Time to wait before retrying a failed server

        # To add more workers just add more server lines:
        # server worker2:8000 max_fails=3 fail_timeout=30s;
        # server worker3:8000 max_fails=3 fail_timeout=30s;

        # Health check settings
        keepalive 32; # Number of idle connections to keep alive to backend (connection pooling)
}

    # --------------------------------------------
    # Server Block
    # Handles incoming HTTP requests
    # --------------------------------------------
    server {
        listen 80; # Listen on port 80 for HTTP
        server_name _; # Accept requests from any domain

        # Maximum request body size (for model uploads)
        client_max_body_size 100M;

        # ----------------------------------------
        # Health Check Endpoint (Nginx itself)
        # ----------------------------------------
        location /nginx-health {
            access_log off; # Don't log health check requests
            return 200 'healthy\n'; # Respond with 200 OK for health checks
            add_header Content-Type text/plain; 
            # This is separate from worker health checks
            # Useful for load balancer health checks pointing to nginx
        }
        # --------------------------------------------
        # Proxy to ML Workers
        # All other requests go to the worker pool
        # --------------------------------------------
        location / {
            # Proxy to the upstream pool
            proxy_pass http://ml_workers;
            # This forwards requests to one of the servers defined in the upstream block

            # Preserve original request information
            proxy_set_header Host $host;
            # $host: Original Host header from client

            proxy_set_header X-Real-IP $remote_addr;
            # Pass real client IP (not the IP of the nginx server)

            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            # Chain of proxies (if multiple load balancers)

            proxy_set_header X-Forwarded-Proto $scheme;
            # http or https (when SSL termination happens elsewhere)

            # Timeouts
            proxy_connect_timeout 60s; # Time to establish connection to backend
            proxy_send_timeout 60s; # Time to send request to backend
            proxy_read_timeout 60s; # Time to wait for response from backend
            # Increase if model inference is slow

            # Buffering (for large responses)
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;

            # Connection pooling to backends
            proxy_http_version 1.1; # Use HTTP/1.1 for keep-alive
            proxy_set_header Connection "";
        }

        # ----------------------------------------
        # Error Pages
        # ----------------------------------------
        error_page 500 502 503 504 /50x.html;
        location = /50x.html {
            root /usr/share/nginx/html;
            # 502 Bad Gateway: Backend is down
            # 503 Service Unavailable: Nginx is overloaded
            # 504 Gateway Timeout: Backend took too long to respond
        }
    }
}

